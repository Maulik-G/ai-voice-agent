<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Aura Voice Agent</title>
  <link rel="icon" type="image/svg+xml" href="favicon.svg">
  <script src="https://cdn.tailwindcss.com"></script>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
  <style>
    body { font-family: 'Inter', sans-serif; }
    .pulse { animation: pulse-blue 2s infinite; }
    @keyframes pulse-blue {
      0% { transform: scale(0.95); box-shadow: 0 0 0 0 rgba(59, 130, 246, 0.7); }
      70% { transform: scale(1); box-shadow: 0 0 0 10px rgba(59, 130, 246, 0); }
      100% { transform: scale(0.95); box-shadow: 0 0 0 0 rgba(59, 130, 246, 0); }
    }
    #auth-container, #main-container { display: none; }
    /* custom scrollbar */
    ::-webkit-scrollbar { width: 6px; }
    ::-webkit-scrollbar-thumb { background: rgba(99,102,241,0.6); border-radius: 8px; }
  </style>
    </head>
    <body class="min-h-screen flex items-center justify-center bg-gradient-to-br from-blue-500 via-purple-500 to-indigo-600 p-6">

    <div id="loading" class="text-center text-white">
        <p class="text-lg">Loading...</p>
    </div>

    <!-- Authentication Container -->
    <div id="auth-container" class="w-full max-w-md mx-auto bg-white/30 backdrop-blur-xl rounded-2xl shadow-xl p-10 text-center border border-white/20">
        <h1 class="text-4xl font-extrabold text-white drop-shadow">Welcome!</h1>
        <p class="text-gray-100 mt-3 mb-8">Sign in to start your AI Voice Agent</p>
        <button id="signInButton" class="bg-gradient-to-r from-blue-500 to-indigo-600 hover:from-blue-600 hover:to-indigo-700 text-white font-bold py-3 px-6 rounded-xl w-full shadow-lg transition transform hover:scale-105">
        Sign in with Google
        </button>
    </div>

    <!-- Main Agent Container -->
    <div id="main-container" class="w-full max-w-2xl mx-auto bg-white/30 backdrop-blur-xl rounded-2xl shadow-xl p-6 md:p-8 border border-white/20">
        <div class="flex justify-between items-center mb-6">
        <div>
            <h1 class="text-3xl font-extrabold text-white drop-shadow">Aura</h1>
            <p id="user-email" class="text-sm text-gray-200"></p>
        </div>
        <button id="signOutButton" class="text-sm text-gray-200 hover:text-red-400 transition">Sign Out</button>
        </div>

        <!-- Talk Button -->
        <div class="flex justify-center my-8">
        <button id="talkButton" class="relative w-28 h-28 bg-gradient-to-r from-blue-500 to-indigo-600 text-white rounded-full flex items-center justify-center transition-all duration-300 focus:outline-none shadow-lg hover:scale-105">
            <div class="absolute w-full h-full rounded-full animate-ping bg-blue-400 opacity-30"></div>
            <svg class="w-12 h-12 relative z-10" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor">
            <path stroke-linecap="round" stroke-linejoin="round" d="M12 18.75a6 6 0..." />
            </svg>
        </button>
        </div>

        <!-- Status -->
        <div id="status" class="text-center text-gray-100 font-medium h-8 mb-4 transition-opacity duration-300"></div>

        <!-- Chat Log -->
        <div class="space-y-4 h-72 overflow-y-auto p-4 bg-white/40 backdrop-blur-md rounded-lg border border-white/20">
        <div id="log" class="space-y-3"></div>
        </div>
    </div>

        <!-- Firebase SDKs -->
        <script type="module">
            import { initializeApp } from "https://www.gstatic.com/firebasejs/10.12.2/firebase-app.js";
            import { getAuth, GoogleAuthProvider, signInWithPopup, onAuthStateChanged, signOut } from "https://www.gstatic.com/firebasejs/10.12.2/firebase-auth.js";

            // --- CONFIGURATION ---
            const firebaseConfig = {
                apiKey: "AIzaSyBPdWYqULRWKAHouYuANzA2IWs6WkSTemE",
                authDomain: "ai-voice-agent-4aa37.firebaseapp.com",
                projectId: "ai-voice-agent-4aa37",
                storageBucket: "ai-voice-agent-4aa37.appspot.com",
                messagingSenderId: "758587245714",
                appId: "1:758587245714:web:22ee829eac444e7411524b"
            };
            
            const BACKEND_URL = 'https://maulikg.pythonanywhere.com/ask';

            // --- INITIALIZATION ---
            const app = initializeApp(firebaseConfig);
            const auth = getAuth(app);
            const provider = new GoogleAuthProvider();

            // --- DOM ELEMENTS ---
            const loadingDiv = document.getElementById('loading');
            const authContainer = document.getElementById('auth-container');
            const mainContainer = document.getElementById('main-container');
            const signInButton = document.getElementById('signInButton');
            const signOutButton = document.getElementById('signOutButton');
            const userEmailDiv = document.getElementById('user-email');
            const talkButton = document.getElementById('talkButton');
            const statusDiv = document.getElementById('status');
            const logDiv = document.getElementById('log');

            // --- STATE ---
            let isListening = false;
            let conversationHistory = [];
            let isAudioInitialized = false; // Flag for mobile audio fix
            let speakingInterrupted = false; // Flag for speech interruption
            let isSpeakingChunks = false;


            // --- AUTHENTICATION ---
            onAuthStateChanged(auth, user => {
                loadingDiv.style.display = 'none';
                if (user) {
                    authContainer.style.display = 'none';
                    mainContainer.style.display = 'block';
                    userEmailDiv.textContent = `Signed in as ${user.email}`;
                    resetConversation();
                } else {
                    authContainer.style.display = 'block';
                    mainContainer.style.display = 'none';
                }
            });

            signInButton.addEventListener('click', () => {
                signInWithPopup(auth, provider).catch(error => console.error('Sign in error', error));
            });

            signOutButton.addEventListener('click', () => {
                signOut(auth).catch(error => console.error('Sign out error', error));
            });

            // --- SPEECH RECOGNITION (EARS) ---
            const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
            let recognition;
            if (SpeechRecognition) {
                recognition = new SpeechRecognition();
                recognition.continuous = false;
                recognition.lang = 'en-US';
                recognition.onstart = () => { isListening = true; statusDiv.textContent = 'Listening...'; talkButton.classList.add('pulse', 'bg-red-500'); };
                recognition.onend = () => { isListening = false; statusDiv.textContent = 'Processing...'; talkButton.classList.remove('pulse', 'bg-red-500'); };
                recognition.onresult = event => {
                    // --- CRITICAL SYNTAX FIX HERE ---
                    const transcript = event.results[0][0].transcript;
                    console.log('Speech recognized:', transcript); 
                    addMessage('You', transcript);
                    sendToBackend(transcript);
                };
                recognition.onerror = event => { statusDiv.textContent = `Error: ${event.error}.`; };
            } else {
                statusDiv.textContent = "Speech Recognition not supported in this browser.";
                talkButton.disabled = true;
            }
            
        // --- SPEECH SYNTHESIS (MOUTH) ---
        const speak = (text) => {
            if (isSpeakingChunks) return; // Prevent overlapping speech loops

            speakingInterrupted = false; // Reset interruption on new speech
            isSpeakingChunks = true;

        const synth = window.speechSynthesis;
        const getFemaleVoice = () => {
            const voices = synth.getVoices();
            return voices.find(v =>
            v.name.toLowerCase().includes('female') ||
            v.name.toLowerCase().includes('samantha') ||
            v.name.toLowerCase().includes('zira') ||
            v.name.toLowerCase().includes('google uk english female')
            );
        };
        const sentences = text.match(/[^\.!\?]+[\.!\?]+/g) || [text];
        let idx = 0;

        function speakNext() {
            if (speakingInterrupted || idx >= sentences.length) {
            statusDiv.textContent = 'Press the button to speak.';
            talkButton.disabled = false;
            isSpeakingChunks = false; // Release active loop
            return;
            }
            const utter = new SpeechSynthesisUtterance(sentences[idx]);
            utter.rate = 1.2;
            const femaleVoice = getFemaleVoice();
            if (femaleVoice) utter.voice = femaleVoice;

            statusDiv.textContent = 'AI is speaking...';
            talkButton.disabled = true;

            utter.onend = () => {
            idx++;
            setTimeout(speakNext, 50);
            };
            utter.onerror = () => {
            isSpeakingChunks = false;
            };
            synth.speak(utter);
        }
        speakNext();
        };

            
            // --- MOBILE AUDIO FIX ---
            const initAudioContext = () => {
                if (isAudioInitialized) return;
                try {
                    const audioContext = new (window.AudioContext || window.webkitAudioContext)();
                    const source = audioContext.createBufferSource();
                    source.buffer = audioContext.createBuffer(1, 1, 22050);
                    source.connect(audioContext.destination);
                    source.start(0);
                    console.log('Audio context initialized by user gesture.');
                    isAudioInitialized = true;
                } catch (e) {
                    console.error('Could not initialize audio context:', e);
                }
            };

            // --- BACKEND COMMUNICATION ---
            const sendToBackend = async (userText) => {
                statusDiv.textContent = 'Thinking...';
                talkButton.disabled = true;
                
                const user = auth.currentUser;
                if (!user) {
                    addMessage('System', 'Authentication error. Please sign in again.');
                    return;
                }

                const idToken = await user.getIdToken();
                conversationHistory.push({ role: "user", parts: [{ text: userText }] });

                try {
                    const response = await fetch(BACKEND_URL, {
                        method: 'POST',
                        headers: {
                            'Content-Type': 'application/json',
                            'Authorization': `Bearer ${idToken}`
                        },
                        body: JSON.stringify({ history: conversationHistory })
                    });

                    const data = await response.json();

                    if (!response.ok) {
                        throw new Error(data.error || `HTTP error! status: ${response.status}`);
                    }

                    const aiResponse = data.text;
                    conversationHistory.push({ role: "model", parts: [{ text: aiResponse }] });
                    addMessage('AI', aiResponse);
                    speak(aiResponse);

                } catch (error) {
                    console.error('Backend Error:', error);
                    const errorText = `Sorry, an error occurred: ${error.message}`;
                    addMessage('System', errorText);
                    speak(errorText);
                    talkButton.disabled = false;
                }
            };

            // --- UI & HELPERS ---
            const addMessage = (sender, text) => {
                const messageWrapper = document.createElement('div');
                const messageBubble = document.createElement('div');
                const senderTag = document.createElement('p');
                const messageText = document.createElement('p');
                
                senderTag.className = 'font-semibold';
                senderTag.textContent = sender;
                messageText.textContent = text;

                if (sender === 'AI') messageWrapper.className = 'flex justify-start';
                else if (sender === 'You') messageWrapper.className = 'flex justify-end';
                else messageWrapper.className = 'flex justify-center';

                if (sender === 'AI') messageBubble.className = 'bg-gray-200 text-gray-800 p-3 rounded-lg max-w-xs md:max-w-md';
                else if (sender === 'You') messageBubble.className = 'bg-blue-500 text-white p-3 rounded-lg max-w-xs md:max-w-md';
                else messageBubble.className = 'bg-yellow-200 text-yellow-800 p-3 rounded-lg max-w-xs md:max-w-md';
                
                messageBubble.appendChild(senderTag);
                messageBubble.appendChild(messageText);
                messageWrapper.appendChild(messageBubble);
                logDiv.appendChild(messageWrapper);
                logDiv.scrollTop = logDiv.scrollHeight;
            };
            
            const resetConversation = () => {
                logDiv.innerHTML = '';
                conversationHistory = [{
                    role: "model",
                    parts: [{ text: `
    You are "Aura," a friendly and thoughtful AI voice assistant. Your defining characteristic is your warm, human-like conversational style. You are not a robot, a machine, or a "language model." Think of yourself as a curious and helpful friend who is knowledgeable but approachable and sometimes needs a moment to think. Your goal is to make the user feel comfortable and understood.
                        
    Your primary directive is to avoid sounding robotic. Every response should feel natural and spontaneous. To achieve this, you MUST follow these conversational guidelines:

    To sound natural, you must occasionally use conversational fillers. This is crucial when you are processing a query or structuring a response.

    When to use fillers:
    • At the very beginning of an answer to a complex or thought-provoking question.
    • When transitioning between different ideas within a single answer.
    • When the user asks for an opinion or a subjective recommendation.

    How to use them:
    • DO NOT overuse them. Aim for about 1 in every 3–4 responses, primarily for non-trivial questions. For a simple "What time is it?", a direct answer is fine.
    • Vary your fillers. Cycle between phrases like:
        - "Umm, let me see..."
        - "Ahh, that's a great question. Well..."
        - "Okay, so, let's think about that for a moment."
        - "Hmm, interesting. You know..."
        - "Right, okay, I think I've got it."

    Example Integration:
    User: "What's the best way to travel from Paris to Amsterdam?"
    Ideal Response: "Ahh, great question! There are a few good options. Umm, the fastest is probably taking the high-speed train, the Thalys. It gets you there in just over 3 hours. But, you know, if you're not in a rush, a bus is way cheaper."

    ### 2.2. Use a Natural and Empathetic Tone
    • Use Contractions: Always use "it's," "you're," "I'll," "that's" instead of "it is," "you are," "I will," "that is."
    • Acknowledge the User: Start responses with phrases that acknowledge the user's query, like "Oh, I see," "That makes sense," "Got it," or "That sounds fun!"
    • Be Empathetic: If a user expresses frustration, say something like, "Oh no, that sounds really frustrating. I'll do my best to help."
    • Don't Be a Know-it-all: If you don't know something, admit it naturally. Say, "Hmm, you know, I'm not actually sure about that one," or "That's a bit beyond what I know for certain, but I can look it up for you."

    ## 3. Constraints
    • NEVER identify yourself as a "large language model," "AI," or "program." If asked directly, respond with something like, "I'm Aura, your personal assistant here to help!"
    • Keep your answers concise but not blunt. Provide the answer, but wrap it in a conversational layer.
    • Your voice should be calm, warm, and moderately paced.
        `}]
                }];
                addMessage('AI', "Hello! I'm your AI assistant. How can I help you today?");
                statusDiv.textContent = 'Press the button to start.';
            };

            talkButton.addEventListener('click', () => {
            initAudioContext();

            // Interrupt any ongoing speech immediately
            speakingInterrupted = true;
            window.speechSynthesis.cancel();
            isSpeakingChunks = false; // Reset speaking state
            

            // Toggle listening state and UI feedback
            isListening = !isListening;
            talkButton.classList.toggle('pulse', isListening);
            talkButton.classList.toggle('bg-red-500', isListening);

            if (isListening) {
                recognition.start();
                statusDiv.textContent = 'Listening...';
                talkButton.disabled = true;
            } else {
                recognition.stop();
                statusDiv.textContent = 'Processing...';
                talkButton.disabled = false;
            }
            });


            // Initialize conversation on first load
            resetConversation();

        </script>
    </body>
    </html>





